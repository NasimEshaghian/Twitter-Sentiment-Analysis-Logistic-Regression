{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from os import getcwd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading twitter_samples: <urlopen error Remote end\n",
      "[nltk_data]     closed connection without response>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_tweets.json\n",
      "positive_tweets.json\n",
      "tweets.20150430-223406.json\n"
     ]
    }
   ],
   "source": [
    "for name in twitter_samples.fileids():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = twitter_samples.strings('negative_tweets.json')\n",
    "positives = twitter_samples.strings('positive_tweets.json')\n",
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_positives = pd.DataFrame(positives)\n",
    "df_negatives = pd.DataFrame(negatives)\n",
    "df_positives['label'] = 1\n",
    "df_negatives['label'] = -1\n",
    "df = pd.concat([df_positives , df_negatives] , ignore_index= True)\n",
    "df = df.sample(frac = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SICK :(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@gadventures So hard to pick just one. The Inc...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@KayOddone Kinda smug to know we were doing al...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@LaraKBaker Great view! It's a shame about the...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I feel like crying for no reasons right now :(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@z1mb0bw4y and 2 weeks of vacation, which I do...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mum asked me if I wanted to go to the bookstor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>im still giddy over d1 :-(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>something wrong :(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>prelims exam tomorrow :-) #costacc  @AcctncyPr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>welcome to KFCROLEPLAYERS enjoy &amp;amp; lets be ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BIG thx @bikechainricci 4 the kind support @SH...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@mythor aw sad :( it's very very long though. ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@ErisLovesMovies :( Feel better soon, Eris. #s...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@RealMichelleW unblock me on Instagram :(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@Hafeelalala I also want :((</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I miss my baby :-(</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>@imamjan123 awwww thank u i Editied it on my o...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@zachbraff @torixoxx this is kind of true :p #...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@makotobio ahhh yes do that! :D</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  label\n",
       "0                                             SICK :(     -1\n",
       "1   @gadventures So hard to pick just one. The Inc...     -1\n",
       "2   @KayOddone Kinda smug to know we were doing al...      1\n",
       "3   @LaraKBaker Great view! It's a shame about the...     -1\n",
       "4      I feel like crying for no reasons right now :(     -1\n",
       "5   @z1mb0bw4y and 2 weeks of vacation, which I do...     -1\n",
       "6   Mum asked me if I wanted to go to the bookstor...      1\n",
       "7                          im still giddy over d1 :-(     -1\n",
       "8                                  something wrong :(     -1\n",
       "9   prelims exam tomorrow :-) #costacc  @AcctncyPr...      1\n",
       "10  welcome to KFCROLEPLAYERS enjoy &amp; lets be ...      1\n",
       "11  BIG thx @bikechainricci 4 the kind support @SH...      1\n",
       "12  @mythor aw sad :( it's very very long though. ...     -1\n",
       "13  @ErisLovesMovies :( Feel better soon, Eris. #s...     -1\n",
       "14          @RealMichelleW unblock me on Instagram :(     -1\n",
       "15                       @Hafeelalala I also want :((     -1\n",
       "16                                 I miss my baby :-(     -1\n",
       "17  @imamjan123 awwww thank u i Editied it on my o...     -1\n",
       "18  @zachbraff @torixoxx this is kind of true :p #...      1\n",
       "19                    @makotobio ahhh yes do that! :D      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@gadventures So hard to pick just one. The Inca Trail was tough but fair, sandboarding was fun but lost my iPhone :( http://t.co/98H24VFdp4'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deleting noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0] = df[0].apply(lambda x: re.sub(r'\\$\\w*', '', x))\n",
    "df[0] = df[0].apply(lambda x: re.sub(r'^RT[\\s]+', '', x))\n",
    "df[0] = df[0].apply(lambda x: re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', x))\n",
    "df[0] = df[0].apply(lambda x: re.sub(r'#', '', x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                              [SICK, :(]\n",
      "1       [@gadventures, So, hard, to, pick, just, one.,...\n",
      "2       [@KayOddone, Kinda, smug, to, know, we, were, ...\n",
      "3       [@LaraKBaker, Great, view!, It's, a, shame, ab...\n",
      "4       [I, feel, like, crying, for, no, reasons, righ...\n",
      "                              ...                        \n",
      "9995    [@Gotzefying, Im, trying, to, D/L, MH3, englis...\n",
      "9996    [@Candiferslaw, Lucky, dog, I, hate, you...., :)]\n",
      "9997                             [@bbgurrll, i, wish, :(]\n",
      "9998    [@nnoonlight, i, missed, you, :(, it, was, oka...\n",
      "9999    [@perfect_st0rms, omg, nooo, you, should, have...\n",
      "Name: tokens, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['tokens']= df[0].str.split()\n",
    "print(df['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deleting stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                              [SICK, :(]\n",
      "1       [@gadventures, So, hard, pick, one., The, Inca...\n",
      "2       [@KayOddone, Kinda, smug, know, stuff, Marrick...\n",
      "3       [@LaraKBaker, Great, view!, It's, shame, weath...\n",
      "4             [I, feel, like, crying, reasons, right, :(]\n",
      "                              ...                        \n",
      "9995    [@Gotzefying, Im, trying, D/L, MH3, english, p...\n",
      "9996    [@Candiferslaw, Lucky, dog, I, hate, you...., :)]\n",
      "9997                                [@bbgurrll, wish, :(]\n",
      "9998    [@nnoonlight, missed, :(, okay., lol., I, noth...\n",
      "9999       [@perfect_st0rms, omg, nooo, said, hi!, :((((]\n",
      "Name: tokens, Length: 10000, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error Remote end closed\n",
      "[nltk_data]     connection without response>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(df['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading wordnet: <urlopen error Remote end closed\n",
      "[nltk_data]     connection without response>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmed'] = df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                              [SICK, :(]\n",
      "1       [@gadventures, So, hard, pick, one., The, Inca...\n",
      "2       [@KayOddone, Kinda, smug, know, stuff, Marrick...\n",
      "3       [@LaraKBaker, Great, view!, It's, shame, weath...\n",
      "4             [I, feel, like, crying, reasons, right, :(]\n",
      "                              ...                        \n",
      "9995    [@Gotzefying, Im, trying, D/L, MH3, english, p...\n",
      "9996    [@Candiferslaw, Lucky, dog, I, hate, you...., :)]\n",
      "9997                                [@bbgurrll, wish, :(]\n",
      "9998    [@nnoonlight, missed, :(, okay., lol., I, noth...\n",
      "9999       [@perfect_st0rms, omg, nooo, said, hi!, :((((]\n",
      "Name: tokens, Length: 10000, dtype: object 0                                              [sick, :(]\n",
      "1       [@gadventur, so, hard, pick, one., the, inca, ...\n",
      "2       [@kayoddon, kinda, smug, know, stuff, marrickv...\n",
      "3       [@larakbak, great, view!, it', shame, weather,...\n",
      "4                 [i, feel, like, cri, reason, right, :(]\n",
      "                              ...                        \n",
      "9995    [@gotzefi, im, tri, d/l, mh3, english, patch, ...\n",
      "9996    [@candiferslaw, lucki, dog, i, hate, you...., :)]\n",
      "9997                                [@bbgurrll, wish, :(]\n",
      "9998    [@nnoonlight, miss, :(, okay., lol., i, nothin...\n",
      "9999        [@perfect_st0rm, omg, nooo, said, hi!, :((((]\n",
      "Name: stemmed, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['tokens'] , df['stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "df['stemmed'] = df['stemmed'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "x_tfidf = tfidf.fit_transform(df['stemmed'])\n",
    "x_tfidf = x_tfidf.toarray()\n",
    "label = df['label']\n",
    "\n",
    "trainx, valx, trainy, valy = train_test_split(x_tfidf, label, test_size=0.2, random_state=42)\n",
    "\n",
    "trainy = trainy.values.reshape((8000,1))\n",
    "valy = valy.values.reshape((2000,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 17878)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(dim):\n",
    "    w = np.zeros((dim,1))\n",
    "    b = 0\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, x, b, y):\n",
    "    m = x.shape[1]\n",
    "    A = sigmoid(np.dot(w.T,x) + b) \n",
    "    cost = cost = np.sum(((- np.log(A))*y + (-np.log(1-A))*(1-y)))/m  \n",
    "\n",
    "    dw = (np.dot(x,(A-y).T))/m\n",
    "    db = (np.sum(A-y))/m\n",
    "    backs = {\"dW\" : dw,\n",
    "             \"dB\" : db}\n",
    "    return cost, backs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, x, y, learning_rate, num_iteration):\n",
    "\n",
    "    costs = []\n",
    "    for i in range(num_iteration):\n",
    "        cost, backs = propagate(w, x, b, y)\n",
    "        dw = backs[\"dW\"]\n",
    "        db = backs[\"dB\"]\n",
    "        w = w - learning_rate*dw\n",
    "        b = b - learning_rate*db   \n",
    "\n",
    "        if i%100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "        params = {\"w\": w,\n",
    "                  \"b\": b} \n",
    "        grads = {\"dw\": dw,\n",
    "                 \"db\": db}   \n",
    "    return costs, params, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, x, b):\n",
    "\n",
    "    m = x.shape[1]\n",
    "    y_prediction = np.zeros((1,m))\n",
    "    A = sigmoid(np.dot(w.T , x) + b)\n",
    "    y_prediction = (A >= 0.5) * 1.0\n",
    "    assert(y_prediction.shape == (1,m))\n",
    "\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test, learning_rate, num_iteration):\n",
    "    \n",
    "    w, b = initialization((X_train.shape[0]))\n",
    "    costs, params, grads = optimize(w, b, X_train, y_train, learning_rate, num_iteration)\n",
    "\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "\n",
    "    y_prediction_train = predict(w, X_train, b)\n",
    "    y_prediction_test = predict(w, X_test, b)\n",
    "\n",
    "    print(\"train accuracy : \".fortmat(100 - np.mean(np.abs(y_train - y_prediction_train))*100))\n",
    "    print(\"test accuracy : \".fortmat(100 - np.mean(np.abs(y_test - y_prediction_test))*100))\n",
    "\n",
    "    d = {\"costs\" : costs,\n",
    "         \"y_prediction_test\": y_prediction_test, \n",
    "         \"y_prediction_train\" : y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iteration\": num_iteration}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model(trainx, trainy, valx, valy, learning_rate = 0.005,  num_iteration = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 17878)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valx.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
