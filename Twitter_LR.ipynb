{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from os import getcwd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Rayan_Pardaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_tweets.json\n",
      "positive_tweets.json\n",
      "tweets.20150430-223406.json\n"
     ]
    }
   ],
   "source": [
    "for name in twitter_samples.fileids():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = twitter_samples.strings('negative_tweets.json')\n",
    "positives = twitter_samples.strings('positive_tweets.json')\n",
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_positives = pd.DataFrame(positives)\n",
    "df_negatives = pd.DataFrame(negatives)\n",
    "df_positives['label'] = 1\n",
    "df_negatives['label'] = 0\n",
    "df = pd.concat([df_positives , df_negatives] , ignore_index= True)\n",
    "df = df.sample(frac = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "db3e8ed0-748b-4522-834e-9e096521b1f5",
       "rows": [
        [
         "0",
         "@silvslee lol go! It's cheap right now :)",
         "1"
        ],
        [
         "1",
         "i miss 5sos so much i cant deal with waiting possibly a year before i see them again :(",
         "0"
        ],
        [
         "2",
         ":( I went to bed way too late",
         "0"
        ],
        [
         "3",
         "@treybaimbridge @hdez_emmy @IAmAlii1 @Abir_Khusro don't want you to ruin your dreams by thinking you're batman :-)",
         "1"
        ],
        [
         "4",
         "@1Erre I'll take the infection :)",
         "1"
        ],
        [
         "5",
         "@Jaaeeeeee lucky :( I was feeling hungry, but then I didn't wanna get out of bed either, so I gotta wait until ma√±ana",
         "0"
        ],
        [
         "6",
         "@MrsManfyDiston Thanks for following! Do get in touch if you'd like any more info about our project: youth@bipolaruk.org.uk :)",
         "1"
        ],
        [
         "7",
         "What do we reckon are the chances of me seeing any #T20 action in #Taunton tonight??? *hopeful but realistic face* :-(",
         "0"
        ],
        [
         "8",
         "i may look strong, hey :-(",
         "0"
        ],
        [
         "9",
         "@JavsNH sure :)",
         "1"
        ],
        [
         "10",
         "@kerryjackson201 Yippeee! We hope you + your little one enjoy some scrummy recipes :)",
         "1"
        ],
        [
         "11",
         "@F41rygirl @paintingandbook You saying you want Lucy to be gone soon, Lisa!!? Oh that's horrible!\nSorry :-)  \nYes, be back soon, please!! x",
         "1"
        ],
        [
         "12",
         "Hello :) Get Youth Job Opportunities follow &gt;&gt; @tolajobjobs @Kgotsoab",
         "1"
        ],
        [
         "13",
         "where's denise i miss her :(",
         "0"
        ],
        [
         "14",
         "finding old pictures with my long hair makes me so sad :(",
         "0"
        ],
        [
         "15",
         "Why didn't I glo up yet??? :((",
         "0"
        ],
        [
         "16",
         "@roguefond me too :(",
         "0"
        ],
        [
         "17",
         "I wanna go to the movies today but no one is down :(",
         "0"
        ],
        [
         "18",
         "like I can't actually put any pressure on my ankle so I have to hop around the house and I just lost my balance and fell over :(",
         "0"
        ],
        [
         "19",
         "@freekuku_ @Lennethxvii its up :D http://t.co/9CEbRspBdJ",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@silvslee lol go! It's cheap right now :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i miss 5sos so much i cant deal with waiting p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:( I went to bed way too late</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@treybaimbridge @hdez_emmy @IAmAlii1 @Abir_Khu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@1Erre I'll take the infection :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>@Jaaeeeeee lucky :( I was feeling hungry, but ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@MrsManfyDiston Thanks for following! Do get i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What do we reckon are the chances of me seeing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i may look strong, hey :-(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@JavsNH sure :)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@kerryjackson201 Yippeee! We hope you + your l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@F41rygirl @paintingandbook You saying you wan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hello :) Get Youth Job Opportunities follow &amp;g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>where's denise i miss her :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>finding old pictures with my long hair makes m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Why didn't I glo up yet??? :((</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@roguefond me too :(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I wanna go to the movies today but no one is d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>like I can't actually put any pressure on my a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>@freekuku_ @Lennethxvii its up :D http://t.co/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  label\n",
       "0           @silvslee lol go! It's cheap right now :)      1\n",
       "1   i miss 5sos so much i cant deal with waiting p...      0\n",
       "2                       :( I went to bed way too late      0\n",
       "3   @treybaimbridge @hdez_emmy @IAmAlii1 @Abir_Khu...      1\n",
       "4                   @1Erre I'll take the infection :)      1\n",
       "5   @Jaaeeeeee lucky :( I was feeling hungry, but ...      0\n",
       "6   @MrsManfyDiston Thanks for following! Do get i...      1\n",
       "7   What do we reckon are the chances of me seeing...      0\n",
       "8                          i may look strong, hey :-(      0\n",
       "9                                     @JavsNH sure :)      1\n",
       "10  @kerryjackson201 Yippeee! We hope you + your l...      1\n",
       "11  @F41rygirl @paintingandbook You saying you wan...      1\n",
       "12  Hello :) Get Youth Job Opportunities follow &g...      1\n",
       "13                       where's denise i miss her :(      0\n",
       "14  finding old pictures with my long hair makes m...      0\n",
       "15                     Why didn't I glo up yet??? :((      0\n",
       "16                               @roguefond me too :(      0\n",
       "17  I wanna go to the movies today but no one is d...      0\n",
       "18  like I can't actually put any pressure on my a...      0\n",
       "19  @freekuku_ @Lennethxvii its up :D http://t.co/...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i miss 5sos so much i cant deal with waiting possibly a year before i see them again :('"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deleting noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0] = df[0].apply(lambda x: re.sub(r'\\$\\w*', '', x))\n",
    "df[0] = df[0].apply(lambda x: re.sub(r'^RT[\\s]+', '', x))\n",
    "df[0] = df[0].apply(lambda x: re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', x))\n",
    "df[0] = df[0].apply(lambda x: re.sub(r'#', '', x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [@silvslee, lol, go!, It's, cheap, right, now,...\n",
      "1       [i, miss, 5sos, so, much, i, cant, deal, with,...\n",
      "2                  [:(, I, went, to, bed, way, too, late]\n",
      "3       [@treybaimbridge, @hdez_emmy, @IAmAlii1, @Abir...\n",
      "4                [@1Erre, I'll, take, the, infection, :)]\n",
      "                              ...                        \n",
      "9995                                    [Great, News, :)]\n",
      "9996    [@theCIGirl, @LeRoiHaptalon, @stpierjane, @Hea...\n",
      "9997    [@_ESCN0016, @Onatsss, @ljabasula, @coracarona...\n",
      "9998       [@clivewalker, near, enough, to, Dartmoor, :)]\n",
      "9999    [@Hayleychinney, Hope, this, isn't, a, regular...\n",
      "Name: tokens, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df['tokens']= df[0].str.split()\n",
    "print(df['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deleting stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           [@silvslee, lol, go!, It's, cheap, right, :)]\n",
      "1       [miss, 5sos, much, cant, deal, waiting, possib...\n",
      "2                           [:(, I, went, bed, way, late]\n",
      "3       [@treybaimbridge, @hdez_emmy, @IAmAlii1, @Abir...\n",
      "4                     [@1Erre, I'll, take, infection, :)]\n",
      "                              ...                        \n",
      "9995                                    [Great, News, :)]\n",
      "9996    [@theCIGirl, @LeRoiHaptalon, @stpierjane, @Hea...\n",
      "9997    [@_ESCN0016, @Onatsss, @ljabasula, @coracarona...\n",
      "9998           [@clivewalker, near, enough, Dartmoor, :)]\n",
      "9999    [@Hayleychinney, Hope, regular, problem, :(, C...\n",
      "Name: tokens, Length: 10000, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rayan_Pardaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "print(df['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rayan_Pardaz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "stemmer = PorterStemmer()\n",
    "df['stemmed'] = df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           [@silvslee, lol, go!, It's, cheap, right, :)]\n",
      "1       [miss, 5sos, much, cant, deal, waiting, possib...\n",
      "2                           [:(, I, went, bed, way, late]\n",
      "3       [@treybaimbridge, @hdez_emmy, @IAmAlii1, @Abir...\n",
      "4                     [@1Erre, I'll, take, infection, :)]\n",
      "                              ...                        \n",
      "9995                                    [Great, News, :)]\n",
      "9996    [@theCIGirl, @LeRoiHaptalon, @stpierjane, @Hea...\n",
      "9997    [@_ESCN0016, @Onatsss, @ljabasula, @coracarona...\n",
      "9998           [@clivewalker, near, enough, Dartmoor, :)]\n",
      "9999    [@Hayleychinney, Hope, regular, problem, :(, C...\n",
      "Name: tokens, Length: 10000, dtype: object 0             [@silvsle, lol, go!, it', cheap, right, :)]\n",
      "1       [miss, 5so, much, cant, deal, wait, possibl, y...\n",
      "2                           [:(, i, went, bed, way, late]\n",
      "3       [@treybaimbridg, @hdez_emmi, @iamalii1, @abir_...\n",
      "4                         [@1err, i'll, take, infect, :)]\n",
      "                              ...                        \n",
      "9995                                     [great, new, :)]\n",
      "9996    [@thecigirl, @leroihaptalon, @stpierjan, @head...\n",
      "9997    [@_escn0016, @onatsss, @ljabasula, @coracarona...\n",
      "9998             [@clivewalk, near, enough, dartmoor, :)]\n",
      "9999    [@hayleychinney, hope, regular, problem, :(, c...\n",
      "Name: stemmed, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['tokens'] , df['stemmed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=7000, stop_words='english', min_df=2)\n",
    "df['stemmed'] = df['stemmed'].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "x_tfidf = tfidf.fit_transform(df['stemmed'])\n",
    "x_tfidf = x_tfidf.toarray()\n",
    "label = df['label']\n",
    "\n",
    "trainx, valx, trainy, valy = train_test_split(x_tfidf, label, test_size=0.2, random_state=42)\n",
    "\n",
    "trainy = trainy.values.reshape((8000,1))\n",
    "valy = valy.values.reshape((2000,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize Features to bring all TF-IDF values to a similar range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "trainx = scaler.fit_transform(trainx)\n",
    "valx = scaler.transform(valx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 6641)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization(dim):\n",
    "    w = np.random.randn(dim, 1) * 0.01\n",
    "    b = 0\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1/(1+np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, x, b, y):\n",
    "    m = x.shape[1]\n",
    "    A = sigmoid(np.dot(w.T,x) + b) \n",
    "    cost = np.sum(((- np.log(A))*y + (-np.log(1-A))*(1-y)))/m  \n",
    "\n",
    "    dw = (np.dot(x,(A-y).T))/m\n",
    "    db = (np.sum(A-y))/m\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    \n",
    "    backs = {\"dW\" : dw,\n",
    "             \"dB\" : db}\n",
    "    return cost, backs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw = [[0.99845601]\n",
      " [2.39507239]]\n",
      "db = 0.001455578136784208\n",
      "cost = 5.801545319394553\n"
     ]
    }
   ],
   "source": [
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "cost, backs = propagate(w, X, b, Y)\n",
    "print (\"dw = \" + str(backs[\"dW\"]))\n",
    "print (\"db = \" + str(backs[\"dB\"]))\n",
    "print (\"cost = \" + str(cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, x, y, learning_rate, num_iteration):\n",
    "\n",
    "    costs = []\n",
    "    for i in range(num_iteration):\n",
    "        cost, backs = propagate(w, x, b, y)\n",
    "        dw = backs[\"dW\"]\n",
    "        db = backs[\"dB\"]\n",
    "        w = w - (learning_rate*dw)\n",
    "        b = b - (learning_rate*db)  \n",
    "\n",
    "        if i%100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "        params = {\"w\": w,\n",
    "                  \"b\": b} \n",
    "        grads = {\"dw\": dw,\n",
    "                 \"db\": db}   \n",
    "    return costs, params, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[0.19033591]\n",
      " [0.12259159]]\n",
      "b = 1.9253598300845747\n",
      "dw = [[0.67752042]\n",
      " [1.41625495]]\n",
      "db = 0.21919450454067654\n"
     ]
    }
   ],
   "source": [
    "costs, params, grads = optimize(w, b, X, Y, learning_rate = 0.009, num_iteration= 100)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, x, b):\n",
    "\n",
    "    m = x.shape[1]\n",
    "    y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(x.shape[0], 1)\n",
    "    A = sigmoid(np.dot(w.T , x) + b)\n",
    "    y_prediction = (A >= 0.5) * 1.0\n",
    "    assert(y_prediction.shape == (1,m))\n",
    "\n",
    "    return y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions = [[1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w = np.array([[0.1124579],[0.23106775]])\n",
    "b = -0.3\n",
    "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
    "print (\"predictions = \" + str(predict(w, X, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test, learning_rate, num_iteration):\n",
    "    \n",
    "    w, b = initialization((X_train.shape[0]))\n",
    "    costs, params, grads = optimize(w, b, X_train, y_train, learning_rate, num_iteration)\n",
    "\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "\n",
    "    y_prediction_train = predict(w, X_train, b)\n",
    "    y_prediction_test = predict(w, X_test, b)\n",
    "\n",
    "    print(f\"train accuracy : {100 - np.mean(np.abs(y_train - y_prediction_train)) * 100:.6f}\")\n",
    "    print(f\"test accuracy : {100 - np.mean(np.abs(y_test - y_prediction_test)) * 100:.6f}\")\n",
    "\n",
    "    d = {\"costs\" : costs,\n",
    "         \"y_prediction_test\": y_prediction_test, \n",
    "         \"y_prediction_train\" : y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iteration\": num_iteration}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx_transpose = trainx.T\n",
    "trainy_transpose = trainy.T\n",
    "valx_transpose = valx.T\n",
    "valy_transpose = valy.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy : 94.837500\n",
      "test accuracy : 69.650000\n"
     ]
    }
   ],
   "source": [
    "d = model(trainx_transpose, trainy_transpose, valx_transpose, valy_transpose, learning_rate = 0.01,  num_iteration = 5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
